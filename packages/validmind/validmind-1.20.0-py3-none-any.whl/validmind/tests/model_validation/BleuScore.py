# Copyright Â© 2023 ValidMind Inc. All rights reserved.

from dataclasses import dataclass

import evaluate
from nltk.tokenize import word_tokenize

from validmind.vm_models import Metric, ResultSummary, ResultTable, ResultTableMetadata


@dataclass
class BleuScore(Metric):
    """
    Bleu Score
    """

    name = "bleu_score"
    required_inputs = ["model", "model.test_ds"]

    def description(self):
        return """BLEU (Bilingual Evaluation Understudy) emerges as a pivotal algorithm designed to assess the caliber
        of machine-translated text from one natural language to another. It hinges on the pivotal concept of quality,
        which essentially measures the degree of congruence between the output generated by a machine and that of a human
        translator. In essence, BLEU strives to bring machine translations as close as possible to the benchmark of
        a professional human translation. This fundamental idea underpins the essence of BLEU's purpose. Notably, BLEU
        stands as one of the pioneering metrics that staked a claim to a robust correlation with human judgments of
        translation quality. Even today, it continues to maintain its prominence as an automated and cost-effective
        assessment metric, widely embraced within the field of translation evaluation.

        BLEU calculates its scores for individual translated segments, which are typically sentences, by juxtaposing
        them against a set of high-quality reference translations. These individual segment scores are then harmoniously
        amalgamated, painting an insightful picture of the overall quality of the translation across the entire corpus.
        Importantly, it's worth noting that BLEU's evaluation doesn't delve into factors like intelligibility or
        grammatical correctness; instead, it concentrates on aligning the translated text with the benchmark human
        translations. This focus makes BLEU a valuable tool for gauging translation performance in a fast and efficient
        manner.
        """

    def run(self):
        # Load the BLEU evaluation metric
        bleu = evaluate.load("bleu")

        # Compute the BLEU score
        bleu = bleu.compute(
            predictions=self.model.y_test_predict,
            references=self.model.y_test_true,
            tokenizer=word_tokenize,
        )
        return self.cache_results(metric_value={"blue_score_metric": bleu})

    def summary(self, metric_value):
        """
        Build one table for summarizing the bleu score results
        """
        summary_bleu_score = metric_value["blue_score_metric"]

        table = []
        table.append(summary_bleu_score)
        return ResultSummary(
            results=[
                ResultTable(
                    data=table,
                    metadata=ResultTableMetadata(title="Bleu score Results"),
                ),
            ]
        )
