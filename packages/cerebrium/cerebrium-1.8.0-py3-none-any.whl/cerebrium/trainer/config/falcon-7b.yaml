%YAML 1.2
---
training_type: "transformer" # Type of training to run. Either "diffuser" or "transformer".

name: your-falcon7b-name-here # Name of the experiment.
api_key: Your Cerebrium API key here

# Model params:
hf_model_path: "tiiuae/falcon-7b"
model_type: "AutoModelForCausalLM"
dataset_path: path/to/your/local/JSON/dataset.json
custom_tokenizer: "" # custom tokenizer from AutoTokenizer if required.
seed: 42 # random seed for reproducibility.
log_level: "INFO" # log_level level for logging.

# Training params:
training_args:
  num_train_epochs: 50
  # max_steps: 1000 # an optional if you would like to use steps instead of epochs.
  learning_rate: 2.0e-4
  per_device_train_batch_size: 10
  per_device_eval_batch_size: 10
  warmup_steps: 0
  logging_steps: 10
  gradient_accumulation_steps: 4
  group_by_length: False
  fp16: True
  max_grad_norm: 0.3
  lr_scheduler_type: "constant"

base_model_args: # args for loading in the base model.
  load_in_8bit: True
  device_map: "auto"
  trust_remote_code: True

peft_lora_args: # peft lora args.
  r: 32
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules: ["query_key_value"] # This has to be query_key_value for falcon
  bias: "none"
  task_type: "CAUSAL_LM"

dataset_args:
  # if you would like a custom prompt template it's possible to specify it here as below:
  prompt_template:
    description: "A shorter template to experiment with."
    prompt_input: "### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n"
    prompt_no_input: "### Instruction:\n{instruction}\n\n### Response:\n"
    response_split: "### Response:"
  # otherwise use `prompt_template: "short"`
  # if your dataset has data labelled differently, you can set the labels here:
  instruction_column: "prompt"
  label_column: "completion"
  context_column: "context"
  cutoff_len: 512
  train_val_ratio: 0.9
