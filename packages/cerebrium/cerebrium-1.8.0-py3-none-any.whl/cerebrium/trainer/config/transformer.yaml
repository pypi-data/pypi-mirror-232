%YAML 1.2
---
training_type: { training_type } # Type of training to run. Either "diffuser" or "transformer". In this case, "transformer".
name: { deployment_name } # Your name for the fine-tuning run.

# Model params:
hf_model_path: "YOUR_HUGGINGFACE_MODEL_PATH_HERE" # path to your local HuggingFace model.
model_type: "AutoModelForCausalLM"
dataset_path: path/to/your/dataset.json # path to your local JSON dataset.

###############################################################
#  Optional Parameters
###############################################################
custom_tokenizer: "" # custom tokenizer from AutoTokenizer if required.
seed: 42 # random seed for reproducibility.
log_level: "INFO" # log_level level for logging.
cpu: 2 # Number of CPUs to use for training.
memory: 16 # Memory to use for training in GB.

# Training params:
training_args:
  logging_steps: 10
  per_device_train_batch_size: 15
  per_device_eval_batch_size: 15
  warmup_steps: 0
  gradient_accumulation_steps: 4
  num_train_epochs: 50
  learning_rate: 0.0001
  group_by_length: False

base_model_args: # args for loading in the base model with AutoModelForCausalLM
  load_in_8bit: True
  device_map: "auto"

peft_lora_args: # peft lora args.
  r: 8
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "v_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"

dataset_args:
  prompt_template: "short" # Prompt template to use. Either "short" or "long".
  instruction_column: "prompt" # column  name of your prompt in the dataset.json
  label_column: "completion" # column name of your label/completion in the dataset.json
  context_column: "context" # optional column name of your context in the dataset.json
  cutoff_len: 512 # cutoff length for the prompt.
  train_val_ratio: 0.9 # ratio of training to validation data in the dataset split.
