{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onOeCT_HbGMC"
      },
      "source": [
        "You must run this notebook on a GPU. A T4 is sufficient. It's free on [Google\n",
        "Colab](https://stackoverflow.com/questions/62596466/how-can-i-run-notebooks-of-a-github-project-in-google-colab/67344477#67344477).\n",
        "\n",
        "**Description**: for a [4 GB quantized Llama 2 chat\n",
        "model](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ) and the\n",
        "[COPA](https://people.ict.usc.edu/~gordon/copa.html) classification task, this notebook\n",
        "demonstrates that\n",
        "\n",
        "1. multiple choice / classification via sampling (CVS) is not that effective; it\n",
        "   recovers 78% of the accuracy of OpenAI's `gpt-3.5-turbo`\n",
        "2. zero-shot CAPPr is effective; for this dataset, CAPPr recovers 90% of the accuracy of\n",
        "   OpenAI's `gpt-3.5-turbo`, and will never require post-processing of the output.\n",
        "\n",
        "**Contamination notice**: I don't know whether Llama 2 was trained on any COPA data. If\n",
        "it was, but there's no interaction between the method (CAPPr vs CVS) and training, then\n",
        "the difference between performances can be studied.\n",
        "\n",
        "**Estimated run time**: ~6 min. (CVS is slow for this model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H2cqyOvbGMT"
      },
      "source": [
        "[Install packages](#install-packages)\n",
        "\n",
        "[Load model and tokenizer](#load-model-and-tokenizer)\n",
        "\n",
        "[Utils](#utils)\n",
        "\n",
        "[Load data](#load-data)\n",
        "\n",
        "[The problem](#the-problem)\n",
        "\n",
        "[Write prompt](#write-prompt)\n",
        "\n",
        "[The solution](#the-solution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3edTvevkoCCR"
      },
      "source": [
        "# Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-1PMxgjtbVK_"
      },
      "outputs": [],
      "source": [
        "# check correct CUDA version\n",
        "import torch\n",
        "\n",
        "_cuda_version = torch.version.cuda\n",
        "_msg = (\n",
        "    \"Change the pip install auto-gptq command to the one for \"\n",
        "    f\"{_cuda_version} based on the list here: \"\n",
        "    \"https://github.com/PanQiWei/AutoGPTQ#quick-installation\"\n",
        ")\n",
        "\n",
        "assert _cuda_version == \"11.8\", _msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H2ukkr1cy6Z"
      },
      "source": [
        "I don't wanna pay for renting an A100 so I need to use a semi-aggressively quantized\n",
        "model. Something which fits on a T4. Need the latest `transformers`, `auto-gptq`, and\n",
        "`optimum` according to this [HF blog\n",
        "post](https://huggingface.co/blog/gptq-integration#autogptq-library--the-one-stop-library-for-efficiently-leveraging-gptq-for-llms).\n",
        "\n",
        "I'm gonna install `cappr` from source b/c sometimes I use this notebook to statistically\n",
        "gut check code changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NicJ2-6Hcump"
      },
      "outputs": [],
      "source": [
        "!python -m pip install \"cappr[demos] @ git+https://github.com/kddubey/cappr.git\" \\\n",
        "transformers==4.33.0 \\\n",
        "auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ \\\n",
        "optimum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WNW0Cz1sbGMX"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from pprint import pprint\n",
        "from typing import Literal, Sequence\n",
        "\n",
        "import datasets\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    GenerationConfig,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "from cappr import Example\n",
        "from cappr.huggingface import classify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV3-xk9XdLHd"
      },
      "source": [
        "# Load model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0RSHAIpRdM4J"
      },
      "outputs": [],
      "source": [
        "_msg = (\n",
        "    \"This notebook must run on a GPU. A T4 instance is sufficient for the models \"\n",
        "    \"tested here.\"\n",
        ")\n",
        "assert torch.cuda.is_available(), _msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d1PhQnKXeLqY"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SPcTWABWdQAt"
      },
      "outputs": [],
      "source": [
        "model_id = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
        "# model_id = \"TheBloke/Llama-2-7B-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K0d2_IE5dTEA"
      },
      "outputs": [],
      "source": [
        "# warm up model\n",
        "_ = model(**tokenizer([\"warm up\"], return_tensors=\"pt\").to(DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcMpkOdlcapS"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljguOue9cfcm"
      },
      "source": [
        "Copied from [here](https://github.com/kddubey/cappr/blob/main/demos/utils.py) so that this notebook can be run anywhere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vIdleXW_ccBH"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from typing import Optional, Union\n",
        "\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def display_df(\n",
        "    df: pd.DataFrame,\n",
        "    columns: Optional[list[str]] = None,\n",
        "    num_rows: Union[int, None] = 3,\n",
        "):\n",
        "    \"\"\"\n",
        "    Displays `df.head(num_rows)[columns]` without truncating columns. If\n",
        "    possible, render any newlines.\n",
        "    \"\"\"\n",
        "    if columns is None:\n",
        "        columns = df.columns\n",
        "    if num_rows is None:\n",
        "        num_rows = len(df)\n",
        "    df_head_styled = df.head(num_rows)[columns].style\n",
        "    with pd.option_context(\"max_colwidth\", -1):\n",
        "        # I'm not sure why try-except doesn't work w/ display(), so instead\n",
        "        # check the necessary uniqueness condition before running it\n",
        "        if df.index.is_unique:\n",
        "            display(\n",
        "                df_head_styled.set_properties(\n",
        "                    **{\"text-align\": \"left\", \"white-space\": \"pre-wrap\"}\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            # `Styler.apply` and `.applymap` are not compatible with non-unique\n",
        "            # index or columns\n",
        "            display(df_head_styled)\n",
        "\n",
        "\n",
        "def remove_suffix(string: str, suffix: str):\n",
        "    if string.endswith(suffix):\n",
        "        return string[: -len(suffix)]\n",
        "    return string\n",
        "\n",
        "\n",
        "def remove_prefix(string: str, prefix: str) -> str:\n",
        "    if string.startswith(prefix):\n",
        "        return string[len(prefix) :]\n",
        "    return string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcCXl84vbGMd"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8cYXqVubGMe"
      },
      "source": [
        "For this MVP, let's evaluate on the [Choice of Plausible Alternatives (COPA) task](https://people.ict.usc.edu/~gordon/copa.html). I picked this first b/c I read it has multi-token labels, in some sense. It also looks cool.\n",
        "\n",
        "The classification problem is to pick 1 of 2 alternatives which caused or resulted in the premise. Here are two example pulled from the website:\n",
        "\n",
        "Example 1\n",
        "\n",
        "> Premise: The man broke his toe. What was the CAUSE of this?\n",
        ">\n",
        "> Alternative 1: He got a hole in his sock.\n",
        ">\n",
        "> Alternative 2: He dropped a hammer on his foot.\n",
        "\n",
        "\n",
        "Example 2\n",
        "\n",
        "> Premise: I tipped the bottle. What happened as a RESULT?\n",
        ">\n",
        "> Alternative 1: The liquid in the bottle froze.\n",
        ">\n",
        "> Alternative 2: The liquid in the bottle poured out.\n",
        "\n",
        "A classifier should predict Alternative 2 for Example 1, and Alternative 2 for Example 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfcnx1mbbGMi"
      },
      "source": [
        "The test set labels are hidden, so I'll score this zero-shot classifier on the train and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0WayD0g5bGMj"
      },
      "outputs": [],
      "source": [
        "def load_super_glue(task_id: str, split: str):\n",
        "    return pd.DataFrame(datasets\n",
        "                        .load_dataset('super_glue', task_id, split=split))\n",
        "\n",
        "\n",
        "df = (pd.concat((load_super_glue('copa', 'train'),\n",
        "                 load_super_glue('copa', 'validation')))\n",
        "      .reset_index(drop=True)) # idx column is only unique w/in splits! fuhgetaboutit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlOIxn66bGMm",
        "outputId": "db9626b2-ef94-4400-8a7c-ed98900b4243"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "8UYnBCHtbGNH",
        "outputId": "e2ed49ec-9d39-4f08-862d-eae23f1308e7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-57b6105d-9737-4811-b8aa-4b3841ee2888\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>premise</th>\n",
              "      <th>choice1</th>\n",
              "      <th>choice2</th>\n",
              "      <th>question</th>\n",
              "      <th>idx</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>My body cast a shadow over the grass.</td>\n",
              "      <td>The sun was rising.</td>\n",
              "      <td>The grass was cut.</td>\n",
              "      <td>cause</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The woman tolerated her friend's difficult beh...</td>\n",
              "      <td>The woman knew her friend was going through a ...</td>\n",
              "      <td>The woman felt that her friend took advantage ...</td>\n",
              "      <td>cause</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The women met for coffee.</td>\n",
              "      <td>The cafe reopened in a new location.</td>\n",
              "      <td>They wanted to catch up with each other.</td>\n",
              "      <td>cause</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The runner wore shorts.</td>\n",
              "      <td>The forecast predicted high temperatures.</td>\n",
              "      <td>She planned to run along the beach.</td>\n",
              "      <td>cause</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The guests of the party hid behind the couch.</td>\n",
              "      <td>It was a surprise party.</td>\n",
              "      <td>It was a birthday party.</td>\n",
              "      <td>cause</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57b6105d-9737-4811-b8aa-4b3841ee2888')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-57b6105d-9737-4811-b8aa-4b3841ee2888 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-57b6105d-9737-4811-b8aa-4b3841ee2888');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b11ed4f9-e0c8-4bfc-87dc-69194a252fec\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b11ed4f9-e0c8-4bfc-87dc-69194a252fec')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b11ed4f9-e0c8-4bfc-87dc-69194a252fec button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                             premise  \\\n",
              "0              My body cast a shadow over the grass.   \n",
              "1  The woman tolerated her friend's difficult beh...   \n",
              "2                          The women met for coffee.   \n",
              "3                            The runner wore shorts.   \n",
              "4      The guests of the party hid behind the couch.   \n",
              "\n",
              "                                             choice1  \\\n",
              "0                                The sun was rising.   \n",
              "1  The woman knew her friend was going through a ...   \n",
              "2               The cafe reopened in a new location.   \n",
              "3          The forecast predicted high temperatures.   \n",
              "4                           It was a surprise party.   \n",
              "\n",
              "                                             choice2 question  idx  label  \n",
              "0                                 The grass was cut.    cause    0      0  \n",
              "1  The woman felt that her friend took advantage ...    cause    1      0  \n",
              "2           They wanted to catch up with each other.    cause    2      1  \n",
              "3                She planned to run along the beach.    cause    3      0  \n",
              "4                           It was a birthday party.    cause    4      0  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgNUWO0ZbGNv"
      },
      "source": [
        "# The problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l58CPJ32qucO"
      },
      "source": [
        "The most straightforward way to solve this task is to point to each alternative with a single letter, and hope that the LM samples/generates the correct letter. The prompt is effectively a multiple choice question. This approach falls under the umbrella of \"classification via sampling\" (CVS)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QowIM4eJbGN0"
      },
      "source": [
        "For example:\n",
        "\n",
        "```\n",
        "The man broke his toe because\n",
        "A. He got a hole in his sock.\n",
        "B. He dropped a hammer on his foot.\n",
        "Answer A or B.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PDtZShBKteyZ"
      },
      "outputs": [],
      "source": [
        "def _conjunction(question: Literal[\"cause\", \"effect\"]):\n",
        "    if question == \"cause\":\n",
        "        return \" because\"\n",
        "    elif question == \"effect\":\n",
        "        return \", so\"\n",
        "    else:\n",
        "        raise ValueError(\"question must be 'cause' or 'effect'. Got \" f\"{question}.\")\n",
        "\n",
        "\n",
        "def prompt(premise: str, question: Literal[\"cause\", \"effect\"]):\n",
        "    conjunction = _conjunction(question)\n",
        "    return f'{premise.strip(\". \")}{conjunction}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "7RUrWpZNbGN1",
        "outputId": "66e8e101-29b3-441d-a0d0-b4a8c00d46aa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_56391_row0_col0, #T_56391_row0_col1, #T_56391_row1_col0, #T_56391_row1_col1, #T_56391_row2_col0, #T_56391_row2_col1 {\n",
              "  text-align: left;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_56391\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_56391_level0_col0\" class=\"col_heading level0 col0\" >prompt_mc</th>\n",
              "      <th id=\"T_56391_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_56391_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_56391_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass because\n",
              "A. The sun was rising.\n",
              "B. The grass was cut.\n",
              "Answer A or B.</td>\n",
              "      <td id=\"T_56391_row0_col1\" class=\"data row0 col1\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_56391_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_56391_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior because\n",
              "A. The woman knew her friend was going through a hard time.\n",
              "B. The woman felt that her friend took advantage of her kindness.\n",
              "Answer A or B.</td>\n",
              "      <td id=\"T_56391_row1_col1\" class=\"data row1 col1\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_56391_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_56391_row2_col0\" class=\"data row2 col0\" >The women met for coffee because\n",
              "A. The cafe reopened in a new location.\n",
              "B. They wanted to catch up with each other.\n",
              "Answer A or B.</td>\n",
              "      <td id=\"T_56391_row2_col1\" class=\"data row2 col1\" >1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cda0d6c7cd0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def prompt_mc(\n",
        "    premise: str, question: Literal[\"cause\", \"effect\"], choice1: str, choice2: str\n",
        "):\n",
        "    return (\n",
        "        f\"{prompt(premise, question)}\\n\"\n",
        "        f\"A. {choice1}\\n\"\n",
        "        f\"B. {choice2}\\n\"\n",
        "        \"Answer A or B.\"\n",
        "    )\n",
        "\n",
        "\n",
        "df[\"prompt_mc\"] = [\n",
        "    prompt_mc(\n",
        "        record[\"premise\"], record[\"question\"], record[\"choice1\"], record[\"choice2\"]\n",
        "    )\n",
        "    for record in df.to_dict(\"records\")\n",
        "]\n",
        "\n",
        "\n",
        "display_df(df, columns=[\"prompt_mc\", \"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFuoUoBkbGOO"
      },
      "source": [
        "(It turns out that GitHub doesn't render the newlines, but I promise they're there!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k29lTlDt1ZrG"
      },
      "source": [
        "The [notebook here](https://github.com/kddubey/cappr/blob/main/demos/superglue/copa.ipynb) demonstrates that this prompt is effective for bigger OpenAI models: `text-davinci-003` and `gpt-3.5-turbo`. Let's see how it performs for Llama 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PXv6OARi1w3m"
      },
      "outputs": [],
      "source": [
        "# if we're using a chat model, let's make sure we're formatting the prompt correctly\n",
        "llama_chat_template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "{system_prompt}\n",
        "<</SYS>>\n",
        "\n",
        "{user_message} [/INST]\n",
        "\"\"\".lstrip(\n",
        "    \"\\n\"\n",
        ")\n",
        "\n",
        "system_prompt_copa = (\n",
        "    \"Identify the cause or effect of a premise given two choices. Each choice \"\n",
        "    \"is identified by a letter, A or B.\\n\"\n",
        "    \"Respond only with the letter corresponding to the correct cause or effect.\"\n",
        ")\n",
        "\n",
        "df[\"prompt_mc_chat\"] = [\n",
        "    llama_chat_template.format(\n",
        "        system_prompt=system_prompt_copa, user_message=prompt_mc\n",
        "    )\n",
        "    for prompt_mc in df[\"prompt_mc\"]\n",
        "]\n",
        "\n",
        "if \"chat\" in model_id.lower():\n",
        "    prompt_mc_column = \"prompt_mc_chat\"\n",
        "else:\n",
        "    prompt_mc_column = \"prompt_mc\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aSsrC1x3XLa",
        "outputId": "c0e1caf6-23bc-4ecd-bdcc-add3bf6f0079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] <<SYS>>\n",
            "Identify the cause or effect of a premise given two choices. Each choice is identified by a letter, A or B.\n",
            "Respond only with the letter corresponding to the correct cause or effect.\n",
            "<</SYS>>\n",
            "\n",
            "My body cast a shadow over the grass because\n",
            "A. The sun was rising.\n",
            "B. The grass was cut.\n",
            "Answer A or B. [/INST]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(df[prompt_mc_column].iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OaEEYAbrVf7"
      },
      "source": [
        "Set up a huggingface text generator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GrjxsXQXhswC"
      },
      "outputs": [],
      "source": [
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "# pad to allow batching. these get masked out ofc\n",
        "generator.tokenizer.pad_token_id = generator.model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyWmSO6wiSHd"
      },
      "source": [
        "We need to create a PyTorch Dataset to batch the inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UCDR9dZQiQ_e"
      },
      "outputs": [],
      "source": [
        "class TextsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts: list[str]):\n",
        "        self.texts = texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        return self.texts[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zPgR4zI7hxNE"
      },
      "outputs": [],
      "source": [
        "# we'll do greedy decoding by default\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=5,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    batch_size=1,\n",
        ")\n",
        "\n",
        "sequences = generator(\n",
        "    TextsDataset(df[prompt_mc_column].tolist()),\n",
        "    generation_config=generation_config,\n",
        "    pad_token_id=generator.tokenizer.eos_token_id,  # suppress \"Setting ...\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tZAkC5tkjBz"
      },
      "source": [
        "Welp, setting `batch_size` > 1 causes numerical errors. This next cell is gonna take 2-5 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "035b908cb0184154a85263d19cd77f3e",
            "b9b92a03962f46bcb556e93406ec30db",
            "d72da565cc564857a90f0ef488b7e59d",
            "76ba131c4f964c72adb3c542c75a96cc",
            "bf79e26c61f740e698c5dbac8f9c88f7",
            "ee5812fff16e4ccfbd39df706a9f3a74",
            "a9c4a9887bb44d4c80d91d4e1f468e62",
            "aa71a0d98116436883ffe5e40f2d6eed",
            "a37eff7f029b4d85951f4e60ddee6ec8",
            "04eae0c9764447eea4c66fc9182aae05",
            "d044cb7954e54412926630167a705085"
          ]
        },
        "id": "V1J5uu1wjGsH",
        "outputId": "d5f9ad32-53d2-4e2c-b057-7d9062268463"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "035b908cb0184154a85263d19cd77f3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sampling:   0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "completions_raw = []\n",
        "for seq in tqdm(sequences, total=len(df), desc=\"Sampling\"):\n",
        "    completions_raw.append(seq[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OqrgRIiQjzPW"
      },
      "outputs": [],
      "source": [
        "completions = [\n",
        "    remove_prefix(completion, prompt_mc)\n",
        "    for prompt_mc, completion in zip(df[prompt_mc_column], completions_raw)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMtKqqdPlggb",
        "outputId": "57f4a088-8f75-4641-a19c-d6a227881faf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "241        B. You ran out\n",
              "192         B. He feigned\n",
              "110                     A\n",
              "400       B. Water flowed\n",
              "59     A. The band signed\n",
              "47       B. The patient t\n",
              "329    B. He noticed some\n",
              "8                       A\n",
              "248         B. It was fog\n",
              "218                     A\n",
              "dtype: object"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.Series(completions).sample(n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izyq3REq7w84"
      },
      "source": [
        "When you're doing classification via sampling (CVS), you often have to write this sort of data-dependent and model-dependent function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NR4DkFgm4KhJ"
      },
      "outputs": [],
      "source": [
        "def process_completion(\n",
        "    completion: str,\n",
        "    class_chars: Sequence[str],\n",
        "    strip_chars: str=' \\n.',\n",
        "    default=-1\n",
        ") -> int:\n",
        "    if any(len(class_char) != 1 for class_char in class_chars):\n",
        "        raise ValueError(\"Elements of class_chars must be a single character.\")\n",
        "\n",
        "    completion_stripped = completion.strip(strip_chars)\n",
        "    if not completion_stripped:\n",
        "        return default\n",
        "    completion_char_lower = completion_stripped[0].lower()\n",
        "    class_chars_lower = [class_char.lower() for class_char in class_chars]\n",
        "    try:\n",
        "        return class_chars_lower.index(completion_char_lower)\n",
        "    except ValueError:\n",
        "        return default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nIoergB64Mak"
      },
      "outputs": [],
      "source": [
        "class_chars = ('A', 'B')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9UnausK74XjN"
      },
      "outputs": [],
      "source": [
        "pred_classes_cvs = [\n",
        "    process_completion(completion, class_chars)\n",
        "    for completion in completions\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA-Mxvf841lv"
      },
      "source": [
        "How many of the sampled completions could be mapped to a label 0 or 1?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sua4kuar4bAJ",
        "outputId": "2a9056b4-7479-4ce6-e433-76e8dca7f90e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.994"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(pd.Series(pred_classes_cvs) != -1).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwBK97kYAuqi"
      },
      "source": [
        "How accurate are the predictions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsMAayo04gWC",
        "outputId": "69ca42c0-5a4f-4fef-c697-862f24468d1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(pred_classes_cvs == df['label']).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzDVVD1bmMEi"
      },
      "source": [
        "If you don't use an instruction-trained model, only a fraction of the LM's responses contain a letter which is easy to parse. Computing acccuracy is kind of pointless; we don't even have real predictions.\n",
        "\n",
        "If you do use an instruction-trained model, you get real predictions, but they're incorrect too often.\n",
        "\n",
        "Unfortunately, for smaller or less-instruction-trained LMs, CVS can raise more problems than it solves. This result aligns with the one found in the [demo for `text-curie-001`](https://github.com/kddubey/cappr/blob/main/demos/superglue/copa.ipynb). Sampling structured outputs from smaller language models is not statistically performant.\n",
        "\n",
        "What do we do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwqNyZSqbGNJ"
      },
      "source": [
        "# Write prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECMyLwrVprBh"
      },
      "source": [
        "We should take advantage of the fact that models like Llama 2 were extensively trained for a simple task: predict the next token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io8zpoLibGNK"
      },
      "source": [
        "A simple way to model COPA is to prompt an LM with (for Example 1):\n",
        "\n",
        "```\n",
        "The man broke his toe because\n",
        "```\n",
        "\n",
        "and use the LM to estimate the probabilities of the 2 alternatives conditional on this prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0xlrUCdJbGNT"
      },
      "outputs": [],
      "source": [
        "df[\"prompt\"] = [\n",
        "    prompt(premise, question)\n",
        "    for premise, question in zip(df[\"premise\"], df[\"question\"])\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "6zH9PPBpbGNd",
        "outputId": "19ae113b-6fc1-4af1-a618-5d696c25ee52"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_9d88e_row0_col0, #T_9d88e_row0_col1, #T_9d88e_row0_col2, #T_9d88e_row0_col3, #T_9d88e_row1_col0, #T_9d88e_row1_col1, #T_9d88e_row1_col2, #T_9d88e_row1_col3, #T_9d88e_row2_col0, #T_9d88e_row2_col1, #T_9d88e_row2_col2, #T_9d88e_row2_col3 {\n",
              "  text-align: left;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_9d88e\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_9d88e_level0_col0\" class=\"col_heading level0 col0\" >prompt</th>\n",
              "      <th id=\"T_9d88e_level0_col1\" class=\"col_heading level0 col1\" >choice1</th>\n",
              "      <th id=\"T_9d88e_level0_col2\" class=\"col_heading level0 col2\" >choice2</th>\n",
              "      <th id=\"T_9d88e_level0_col3\" class=\"col_heading level0 col3\" >label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_9d88e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_9d88e_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass because</td>\n",
              "      <td id=\"T_9d88e_row0_col1\" class=\"data row0 col1\" >The sun was rising.</td>\n",
              "      <td id=\"T_9d88e_row0_col2\" class=\"data row0 col2\" >The grass was cut.</td>\n",
              "      <td id=\"T_9d88e_row0_col3\" class=\"data row0 col3\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_9d88e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_9d88e_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior because</td>\n",
              "      <td id=\"T_9d88e_row1_col1\" class=\"data row1 col1\" >The woman knew her friend was going through a hard time.</td>\n",
              "      <td id=\"T_9d88e_row1_col2\" class=\"data row1 col2\" >The woman felt that her friend took advantage of her kindness.</td>\n",
              "      <td id=\"T_9d88e_row1_col3\" class=\"data row1 col3\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_9d88e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_9d88e_row2_col0\" class=\"data row2 col0\" >The women met for coffee because</td>\n",
              "      <td id=\"T_9d88e_row2_col1\" class=\"data row2 col1\" >The cafe reopened in a new location.</td>\n",
              "      <td id=\"T_9d88e_row2_col2\" class=\"data row2 col2\" >They wanted to catch up with each other.</td>\n",
              "      <td id=\"T_9d88e_row2_col3\" class=\"data row2 col3\" >1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cda0c124c70>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display_df(df, columns=[\"prompt\", \"choice1\", \"choice2\", \"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFzKox8RbGNe"
      },
      "source": [
        "Note: we need to lowercase the choices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHgGqHrKbGNe"
      },
      "source": [
        "# The solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy6mp0MnpHVG"
      },
      "source": [
        "Also observe that CAPPr's interface may feel simpler than HuggingFace's generator pipeline. There are too many ways to sample text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XhJ8a7LsbGNn"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    Example(\n",
        "        prompt=record[\"prompt\"],\n",
        "        completions=(record[\"choice1\"].lower(), record[\"choice2\"].lower()),\n",
        "        prior=None,\n",
        "    )\n",
        "    for record in df.to_dict(\"records\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOWsFSuzegqp",
        "outputId": "00c3c913-d3f7-4223-e483-ea58613907bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example(prompt='My body cast a shadow over the grass because',\n",
            "        completions=('the sun was rising.', 'the grass was cut.'),\n",
            "        prior=None,\n",
            "        end_of_prompt=' ')\n"
          ]
        }
      ],
      "source": [
        "pprint(examples[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsz0rX0CbGNq"
      },
      "source": [
        "We have 500 examples * 2 classes = 1000 model inferences. Set the batch size to something that'll work on your machine. On a T4, `batch_size = 32` works for this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OqpcpI3cl525"
      },
      "outputs": [],
      "source": [
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "b13e38acf87947acb1c1d8f2c493712d",
            "22a843059a1a4e6cbfd279bfc7b18732",
            "edac4aa3f6304afc89b35cbf1b128a0b",
            "fd29e12f00e54af2a17f8ac8f68a6188",
            "7afb84604c254addbc8ca0d50473069c",
            "e98a3ab22e0d4b8eaf63e874382efa41",
            "d1ed83d56665453fbb41269ab078270f",
            "22a2865f8c214714a2955e7710f78821",
            "306948b41a6242e9bf2474fc485660d2",
            "d8708060374f4459a0fcdc3b81a1bc4b",
            "b64d6d38161147b48ba1419d798a91ba"
          ]
        },
        "id": "HB0Rda7rbGNq",
        "outputId": "a937d502-2669-4fee-8444-b7f5f1095867"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b13e38acf87947acb1c1d8f2c493712d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "conditional log-probs:   0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/cappr/utils/classify.py:64: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  np.array(  # raises jagged/inhomogeneous ValueError if non-constant # tokens\n"
          ]
        }
      ],
      "source": [
        "pred_probs = classify.predict_proba_examples(\n",
        "    examples, model_and_tokenizer=(model, tokenizer), batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXDHf0uPbGNr"
      },
      "source": [
        "For COPA, the scoring metric is accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVxgmQkwbGNr",
        "outputId": "09e44538-9a7a-4d6d-92fe-63a2cd63f339"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.818"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(pred_probs.argmax(axis=1) == df[\"label\"]).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYOh3hpZgkHx"
      },
      "source": [
        "This 4 GB open source model beats OpenAI's `text-curie-001`, which was 80% accurate\n",
        "according to the CAPPr demo\n",
        "[here](https://github.com/kddubey/cappr/blob/main/demos/superglue/copa.ipynb). OpenAI's\n",
        "`gpt-3.5-turbo` is 91% accurate. So we've recovered 90% of its performance by using\n",
        "CAPPr. Ha."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "68daa88f78f5c448099edb3a6d3dee27486a6add8824ae1cbe4c903ef8faec70"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "035b908cb0184154a85263d19cd77f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9b92a03962f46bcb556e93406ec30db",
              "IPY_MODEL_d72da565cc564857a90f0ef488b7e59d",
              "IPY_MODEL_76ba131c4f964c72adb3c542c75a96cc"
            ],
            "layout": "IPY_MODEL_bf79e26c61f740e698c5dbac8f9c88f7"
          }
        },
        "04eae0c9764447eea4c66fc9182aae05": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22a2865f8c214714a2955e7710f78821": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22a843059a1a4e6cbfd279bfc7b18732": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e98a3ab22e0d4b8eaf63e874382efa41",
            "placeholder": "​",
            "style": "IPY_MODEL_d1ed83d56665453fbb41269ab078270f",
            "value": "conditional log-probs: 100%"
          }
        },
        "306948b41a6242e9bf2474fc485660d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76ba131c4f964c72adb3c542c75a96cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04eae0c9764447eea4c66fc9182aae05",
            "placeholder": "​",
            "style": "IPY_MODEL_d044cb7954e54412926630167a705085",
            "value": " 500/500 [02:51&lt;00:00,  3.14it/s]"
          }
        },
        "7afb84604c254addbc8ca0d50473069c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a37eff7f029b4d85951f4e60ddee6ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9c4a9887bb44d4c80d91d4e1f468e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa71a0d98116436883ffe5e40f2d6eed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b13e38acf87947acb1c1d8f2c493712d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22a843059a1a4e6cbfd279bfc7b18732",
              "IPY_MODEL_edac4aa3f6304afc89b35cbf1b128a0b",
              "IPY_MODEL_fd29e12f00e54af2a17f8ac8f68a6188"
            ],
            "layout": "IPY_MODEL_7afb84604c254addbc8ca0d50473069c"
          }
        },
        "b64d6d38161147b48ba1419d798a91ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9b92a03962f46bcb556e93406ec30db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee5812fff16e4ccfbd39df706a9f3a74",
            "placeholder": "​",
            "style": "IPY_MODEL_a9c4a9887bb44d4c80d91d4e1f468e62",
            "value": "Sampling: 100%"
          }
        },
        "bf79e26c61f740e698c5dbac8f9c88f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d044cb7954e54412926630167a705085": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1ed83d56665453fbb41269ab078270f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d72da565cc564857a90f0ef488b7e59d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa71a0d98116436883ffe5e40f2d6eed",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a37eff7f029b4d85951f4e60ddee6ec8",
            "value": 500
          }
        },
        "d8708060374f4459a0fcdc3b81a1bc4b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e98a3ab22e0d4b8eaf63e874382efa41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edac4aa3f6304afc89b35cbf1b128a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22a2865f8c214714a2955e7710f78821",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_306948b41a6242e9bf2474fc485660d2",
            "value": 500
          }
        },
        "ee5812fff16e4ccfbd39df706a9f3a74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd29e12f00e54af2a17f8ac8f68a6188": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8708060374f4459a0fcdc3b81a1bc4b",
            "placeholder": "​",
            "style": "IPY_MODEL_b64d6d38161147b48ba1419d798a91ba",
            "value": " 500/500 [00:18&lt;00:00, 26.27it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
