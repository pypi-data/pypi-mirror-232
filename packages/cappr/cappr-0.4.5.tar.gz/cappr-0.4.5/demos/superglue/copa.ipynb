{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Description**: demonstrates that the zero-shot text classification method [described\n",
                "here](https://stats.stackexchange.com/q/601159/337906) works well on the [COPA\n",
                "task](https://people.ict.usc.edu/~gordon/copa.html). It's one of the [SuperGLUE\n",
                "tasks](https://super.gluebenchmark.com/tasks) in which labels have multiple tokens, in\n",
                "some sense. An interesting result is that classification-via-sampling using\n",
                "`text-curie-001` (a smaller GPT-3 model) performs worse than random guessing, while\n",
                "CAPPr using `text-curie-001` is 80% accurate.\n",
                "\n",
                "**Contamination notice**: I don't know whether the models used here were trained on any\n",
                "COPA data. If they were, but there's no interaction between the method (CAPPr vs CVS)\n",
                "and training, then the difference between performances can be studied.\n",
                "\n",
                "**Estimated run time**: ~1 min.\n",
                "\n",
                "**Environment**: See the [Setup section in the\n",
                "README](https://github.com/kddubey/cappr/#setup).\n",
                "\n",
                "**Other**: You have to have an OpenAI API key stored in the environment variable\n",
                "`OPENAI_API_KEY`. [Sign up here](https://openai.com/api/). This notebook will manually\n",
                "ask you to give the go-ahead before incurring any costs. Running the whole notebook will\n",
                "cost ya 30 cents.\n",
                "\n",
                "**TODO**: analyze mispredictions."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[Load data](#load-data)\n",
                "\n",
                "[Write prompt](#write-prompt)\n",
                "\n",
                "[Run model](#run-model)\n",
                "\n",
                "[Evaluate CVS](#evaluate-cvs)\n",
                "\n",
                "[Evaluate CVS (chat)](#evaluate-cvs-chat)\n",
                "\n",
                "[Evaluate question](#evaluate-question)\n",
                "\n",
                "[Evaluate single-token](#evaluate-single-token)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from __future__ import annotations\n",
                "import logging\n",
                "import os\n",
                "import sys\n",
                "from typing import Literal, Sequence\n",
                "\n",
                "import datasets as nlp_datasets\n",
                "import pandas as pd\n",
                "\n",
                "from cappr import Example\n",
                "from cappr import openai\n",
                "\n",
                "sys.path.insert(1, os.path.join(sys.path[0], \"..\"))\n",
                "from utils import display_df, remove_prefix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# When hitting the OpenAI endpoints, we'll log any server errors\n",
                "logging.basicConfig(\n",
                "    level=logging.INFO,\n",
                "    handlers=[logging.StreamHandler(stream=sys.stdout)],\n",
                "    format=\"%(asctime)s :: %(name)s :: %(levelname)s :: \" \"%(message)s\",\n",
                ")\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load data"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For this MVP, let's evaluate on the [Choice of Plausible Alternatives (COPA)\n",
                "task](https://people.ict.usc.edu/~gordon/copa.html). I picked this first b/c I read it\n",
                "has multi-token labels, in some sense. It also looks cool.\n",
                "\n",
                "The classification problem is to pick 1 of 2 alternatives which caused or resulted in\n",
                "the premise. Here are two example pulled from the website:\n",
                "\n",
                "Example 1\n",
                "\n",
                "> Premise: The man broke his toe. What was the CAUSE of this?\n",
                ">\n",
                "> Alternative 1: He got a hole in his sock.\n",
                ">\n",
                "> Alternative 2: He dropped a hammer on his foot.\n",
                "\n",
                "\n",
                "Example 2\n",
                "\n",
                "> Premise: I tipped the bottle. What happened as a RESULT?\n",
                ">\n",
                "> Alternative 1: The liquid in the bottle froze.\n",
                ">\n",
                "> Alternative 2: The liquid in the bottle poured out.\n",
                "\n",
                "A classifier should predict Alternative 2 for Example 1, and Alternative 2 for Example\n",
                "2."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The test set labels are hidden, so I'll score this zero-shot classifier on the train and validation sets. We'll be evaluating 5 methods, which is quite a few for only 500 examples. But I didn't tune much of anything."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_super_glue(task_id: str, split: str):\n",
                "    return pd.DataFrame(nlp_datasets\n",
                "                        .load_dataset('super_glue', task_id, split=split))\n",
                "\n",
                "df = (pd.concat((load_super_glue('copa', 'train'),\n",
                "                 load_super_glue('copa', 'validation')))\n",
                "      .reset_index(drop=True)) # the idx column is only unique w/in splits! fuhgetaboutit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "500"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len(df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>premise</th>\n",
                            "      <th>choice1</th>\n",
                            "      <th>choice2</th>\n",
                            "      <th>question</th>\n",
                            "      <th>idx</th>\n",
                            "      <th>label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>My body cast a shadow over the grass.</td>\n",
                            "      <td>The sun was rising.</td>\n",
                            "      <td>The grass was cut.</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>The woman tolerated her friend's difficult beh...</td>\n",
                            "      <td>The woman knew her friend was going through a ...</td>\n",
                            "      <td>The woman felt that her friend took advantage ...</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>The women met for coffee.</td>\n",
                            "      <td>The cafe reopened in a new location.</td>\n",
                            "      <td>They wanted to catch up with each other.</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>2</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>The runner wore shorts.</td>\n",
                            "      <td>The forecast predicted high temperatures.</td>\n",
                            "      <td>She planned to run along the beach.</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>3</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>The guests of the party hid behind the couch.</td>\n",
                            "      <td>It was a surprise party.</td>\n",
                            "      <td>It was a birthday party.</td>\n",
                            "      <td>cause</td>\n",
                            "      <td>4</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                             premise  \\\n",
                            "0              My body cast a shadow over the grass.   \n",
                            "1  The woman tolerated her friend's difficult beh...   \n",
                            "2                          The women met for coffee.   \n",
                            "3                            The runner wore shorts.   \n",
                            "4      The guests of the party hid behind the couch.   \n",
                            "\n",
                            "                                             choice1  \\\n",
                            "0                                The sun was rising.   \n",
                            "1  The woman knew her friend was going through a ...   \n",
                            "2               The cafe reopened in a new location.   \n",
                            "3          The forecast predicted high temperatures.   \n",
                            "4                           It was a surprise party.   \n",
                            "\n",
                            "                                             choice2 question  idx  label  \n",
                            "0                                 The grass was cut.    cause    0      0  \n",
                            "1  The woman felt that her friend took advantage ...    cause    1      0  \n",
                            "2           They wanted to catch up with each other.    cause    2      1  \n",
                            "3                She planned to run along the beach.    cause    3      0  \n",
                            "4                           It was a birthday party.    cause    4      0  "
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Write prompt"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A simple way to model COPA is to prompt an LM with (for Example 1):\n",
                "\n",
                "```\n",
                "The man broke his toe because \n",
                "```\n",
                "\n",
                "and use the LM to estimate the probabilities of the 2 alternatives conditional on this\n",
                "prompt. (See the **Example** section\n",
                "[here](https://stats.stackexchange.com/q/601159/337906) for a full description of what\n",
                "\"estimate the probabilities\" actually means.)\n",
                "\n",
                "This method assumes GPT isn't miscalibrated in bad ways, as it relies entirely on the\n",
                "comparison between averaged probabilities. Another potential issue is that any 2\n",
                "alternatives are gonna have really low probabilities. As a result, discriminating\n",
                "between alternatives may be, numerically and statistically, a bad idea. But that's why\n",
                "this notebook is here: let's see if these issues significantly impact accuracy when\n",
                "compared to classification via sampling (CVS). And even if they do, we could always\n",
                "provide the alternatives in the prompt, as would be done w/ a sampling approach. That's\n",
                "done in [Evaluate single-token](#evaluate-single-token)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def _conjunction(question: Literal[\"cause\", \"effect\"]):\n",
                "    if question == \"cause\":\n",
                "        return \" because\"\n",
                "    elif question == \"effect\":\n",
                "        return \", so\"\n",
                "    else:\n",
                "        raise ValueError(\"question must be 'cause' or 'effect'. Got \" f\"{question}.\")\n",
                "\n",
                "\n",
                "def prompt(premise: str, question: Literal[\"cause\", \"effect\"]):\n",
                "    conjunction = _conjunction(question)\n",
                "    return f'{premise.strip(\". \")}{conjunction}'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[\"prompt\"] = [\n",
                "    prompt(premise, question)\n",
                "    for premise, question in zip(df[\"premise\"], df[\"question\"])\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<style type=\"text/css\">\n",
                            "#T_b60c8_row0_col0, #T_b60c8_row0_col1, #T_b60c8_row0_col2, #T_b60c8_row0_col3, #T_b60c8_row1_col0, #T_b60c8_row1_col1, #T_b60c8_row1_col2, #T_b60c8_row1_col3, #T_b60c8_row2_col0, #T_b60c8_row2_col1, #T_b60c8_row2_col2, #T_b60c8_row2_col3 {\n",
                            "  text-align: left;\n",
                            "  white-space: pre-wrap;\n",
                            "}\n",
                            "</style>\n",
                            "<table id=\"T_b60c8\">\n",
                            "  <thead>\n",
                            "    <tr>\n",
                            "      <th class=\"blank level0\" >&nbsp;</th>\n",
                            "      <th id=\"T_b60c8_level0_col0\" class=\"col_heading level0 col0\" >prompt</th>\n",
                            "      <th id=\"T_b60c8_level0_col1\" class=\"col_heading level0 col1\" >choice1</th>\n",
                            "      <th id=\"T_b60c8_level0_col2\" class=\"col_heading level0 col2\" >choice2</th>\n",
                            "      <th id=\"T_b60c8_level0_col3\" class=\"col_heading level0 col3\" >label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th id=\"T_b60c8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
                            "      <td id=\"T_b60c8_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass because</td>\n",
                            "      <td id=\"T_b60c8_row0_col1\" class=\"data row0 col1\" >The sun was rising.</td>\n",
                            "      <td id=\"T_b60c8_row0_col2\" class=\"data row0 col2\" >The grass was cut.</td>\n",
                            "      <td id=\"T_b60c8_row0_col3\" class=\"data row0 col3\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_b60c8_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
                            "      <td id=\"T_b60c8_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior because</td>\n",
                            "      <td id=\"T_b60c8_row1_col1\" class=\"data row1 col1\" >The woman knew her friend was going through a hard time.</td>\n",
                            "      <td id=\"T_b60c8_row1_col2\" class=\"data row1 col2\" >The woman felt that her friend took advantage of her kindness.</td>\n",
                            "      <td id=\"T_b60c8_row1_col3\" class=\"data row1 col3\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_b60c8_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
                            "      <td id=\"T_b60c8_row2_col0\" class=\"data row2 col0\" >The women met for coffee because</td>\n",
                            "      <td id=\"T_b60c8_row2_col1\" class=\"data row2 col1\" >The cafe reopened in a new location.</td>\n",
                            "      <td id=\"T_b60c8_row2_col2\" class=\"data row2 col2\" >They wanted to catch up with each other.</td>\n",
                            "      <td id=\"T_b60c8_row2_col3\" class=\"data row2 col3\" >1</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<pandas.io.formats.style.Styler at 0x12d465290>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "display_df(df, columns=[\"prompt\", \"choice1\", \"choice2\", \"label\"])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note: we need to lowercase the choices."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Run model"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note that for many SuperGLUE datasets, including COPA, the probability distribution over\n",
                "classes (alternative 1, 2 for COPA) is uniform. So we'll use `prior=None`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "examples = [\n",
                "    Example(\n",
                "        prompt=record[\"prompt\"],\n",
                "        completions=(record[\"choice1\"].lower(), record[\"choice2\"].lower()),\n",
                "        prior=None,\n",
                "    )\n",
                "    for record in df.to_dict(\"records\")\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "500"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len(examples)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We have 500 examples * 2 classes = 1000 OpenAI API requests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8af367028ef0463fba29a9efbd147826",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "log-probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# $0.02\n",
                "pred_probs = openai.classify.predict_proba_examples(\n",
                "    examples, model=\"gpt-3.5-turbo-instruct\", ask_if_ok=True\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For COPA, the scoring metric is accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.9"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_probs.argmax(axis=1) == df[\"label\"]).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To put this number in context, we'll evaluate zero-shot classification via sampling\n",
                "(CVS) on `gpt-3.5-turbo-instruct`.\n",
                "\n",
                "But first, let's see how zero-shot curie performs. Curie is a much smaller model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "96b862651cfe4369beaa4636b0b218c1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "log-probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# $0.03\n",
                "pred_probs_curie = openai.classify.predict_proba_examples(\n",
                "    examples, model=\"text-curie-001\", ask_if_ok=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.802"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_probs_curie.argmax(axis=1) == df[\"label\"]).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "TODO: diagnose these mispredictions. For example, are many caused by differing\n",
                "completion lengths? It's possible that the average likelihood metric is getting thrown\n",
                "off in those casses."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluate CVS"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "COPA isn't a great demo for this approach b/c there's a trivial way to transform\n",
                "multi-token labels to single tokens: just point to each choice with a single letter!\n",
                "\n",
                "For example:\n",
                "\n",
                "```\n",
                "The man broke his toe because\n",
                "A. He got a hole in his sock.\n",
                "B. He dropped a hammer on his foot.\n",
                "Answer A or B.\n",
                "```\n",
                "\n",
                "This prompt is a multiple choice question. And it could probably work well for all of\n",
                "the SuperGLUE tasks, because they're all binary classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<style type=\"text/css\">\n",
                            "#T_89c74_row0_col0, #T_89c74_row0_col1, #T_89c74_row1_col0, #T_89c74_row1_col1, #T_89c74_row2_col0, #T_89c74_row2_col1 {\n",
                            "  text-align: left;\n",
                            "  white-space: pre-wrap;\n",
                            "}\n",
                            "</style>\n",
                            "<table id=\"T_89c74\">\n",
                            "  <thead>\n",
                            "    <tr>\n",
                            "      <th class=\"blank level0\" >&nbsp;</th>\n",
                            "      <th id=\"T_89c74_level0_col0\" class=\"col_heading level0 col0\" >prompt_mc</th>\n",
                            "      <th id=\"T_89c74_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th id=\"T_89c74_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
                            "      <td id=\"T_89c74_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass because\n",
                            "A. The sun was rising.\n",
                            "B. The grass was cut.\n",
                            "Answer A or B.</td>\n",
                            "      <td id=\"T_89c74_row0_col1\" class=\"data row0 col1\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_89c74_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
                            "      <td id=\"T_89c74_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior because\n",
                            "A. The woman knew her friend was going through a hard time.\n",
                            "B. The woman felt that her friend took advantage of her kindness.\n",
                            "Answer A or B.</td>\n",
                            "      <td id=\"T_89c74_row1_col1\" class=\"data row1 col1\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_89c74_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
                            "      <td id=\"T_89c74_row2_col0\" class=\"data row2 col0\" >The women met for coffee because\n",
                            "A. The cafe reopened in a new location.\n",
                            "B. They wanted to catch up with each other.\n",
                            "Answer A or B.</td>\n",
                            "      <td id=\"T_89c74_row2_col1\" class=\"data row2 col1\" >1</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<pandas.io.formats.style.Styler at 0x12c718e10>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "def prompt_mc(\n",
                "    premise: str, question: Literal[\"cause\", \"effect\"], choice1: str, choice2: str\n",
                "):\n",
                "    return (\n",
                "        f\"{prompt(premise, question)}\\n\"\n",
                "        f\"A. {choice1}\\n\"\n",
                "        f\"B. {choice2}\\n\"\n",
                "        \"Answer A or B.\"\n",
                "    )\n",
                "\n",
                "\n",
                "df[\"prompt_mc\"] = [\n",
                "    prompt_mc(\n",
                "        record[\"premise\"], record[\"question\"], record[\"choice1\"], record[\"choice2\"]\n",
                "    )\n",
                "    for record in df.to_dict(\"records\")\n",
                "]\n",
                "\n",
                "\n",
                "display_df(df, columns=[\"prompt_mc\", \"label\"])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "(It turns out that GitHub doesn't render the newlines, but I promise they're there!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6b0c5ea57b644a07b8e562541a25ef37",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "log-probs:   0%|          | 0/500 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# $0.03\n",
                "choices = openai.api.gpt_complete(\n",
                "    df[\"prompt_mc\"],\n",
                "    ask_if_ok=True,\n",
                "    model=\"gpt-3.5-turbo-instruct\",\n",
                "    max_tokens=5,  # need to allow for \"\\n\\nAnswer A\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "completions_mc = [choice[\"text\"] for choice in choices]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_completion(\n",
                "    completion: str,\n",
                "    class_chars: Sequence[str],\n",
                "    prefix_remove: str = \"Answer \",\n",
                "    strip_chars: str = \" \\n.\",\n",
                "    default=-1,\n",
                ") -> int:\n",
                "    if any(len(class_char) != 1 for class_char in class_chars):\n",
                "        raise ValueError(\"Elements of class_chars must be a single character.\")\n",
                "    completion = remove_prefix(completion, prefix_remove)\n",
                "    completion_stripped = completion.strip(strip_chars)\n",
                "    if not completion_stripped:\n",
                "        return default\n",
                "    completion_char_lower = completion_stripped[0].lower()\n",
                "    class_chars_lower = [class_char.lower() for class_char in class_chars]\n",
                "    try:\n",
                "        return class_chars_lower.index(completion_char_lower)\n",
                "    except ValueError:\n",
                "        return default"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "class_chars = (\"A\", \"B\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "pred_classes_cvs = [\n",
                "    process_completion(completion, class_chars) for completion in completions_mc\n",
                "]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Check that most of the sampled completions could be mapped to a label 0 or 1:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.98"
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pd.Series(pred_classes_cvs) != -1).mean()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.908"
                        ]
                    },
                    "execution_count": 22,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_classes_cvs == df[\"label\"]).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This hovers between 0.89 - 0.92 in repeated runs.\n",
                "\n",
                "Let's see how CVS w/ `text-curie-001` performs. Hypothesis: shouldn't be too bad given\n",
                "the curie result above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "def7dc8016a340dca26ef204f94fb7ce",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "log-probs:   0%|          | 0/500 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# $0.04\n",
                "choices_curie = openai.api.gpt_complete(\n",
                "    df[\"prompt_mc\"], ask_if_ok=True, model=\"text-curie-001\", max_tokens=5\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "completions_mc_curie = [choice[\"text\"] for choice in choices_curie]\n",
                "pred_classes_cvs_curie = [\n",
                "    process_completion(completion, class_chars) for completion in completions_mc_curie\n",
                "]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see how many of these sampled completions are actually \"valid\", i.e., in the label set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.614"
                        ]
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "pred_classes_cvs_curie = pd.Series(pred_classes_cvs_curie, index=df.index)\n",
                "(pred_classes_cvs_curie != -1).mean()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.298"
                        ]
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_classes_cvs_curie == df[\"label\"]).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Ouch, much worse than random guessing. Hypothesis very rejected. Let's see how often the\n",
                "valid completions are accurate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.48534201954397393"
                        ]
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "_mask_valid = pred_classes_cvs_curie != -1\n",
                "(pred_classes_cvs_curie[_mask_valid] == df.loc[_mask_valid, \"label\"]).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluate CVS (chat)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "How does the chat completion endpoint perform on COPA? I think it makes sense to use the\n",
                "same prompt as above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "system_prompt_copa = (\n",
                "    \"Identify the cause or effect of a premise given two choices. Each choice \"\n",
                "    \"is identified by a letter, A or B.\\n\"\n",
                "    \"Respond only with the letter corresponding to the correct cause or effect.\"\n",
                ")\n",
                "# getting this right is crucial"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "060a7fc224c544c184babc67cb229974",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Completing chats:   0%|          | 0/500 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2023-09-27 11:20:23,015 :: cappr.openai.api :: INFO :: openai error: The server is overloaded or not ready yet.\n",
                        "2023-09-27 11:20:23,016 :: cappr.openai.api :: INFO :: Try 1. Sleeping for 10 sec.\n",
                        "2023-09-27 11:22:17,213 :: cappr.openai.api :: INFO :: openai error: The server is overloaded or not ready yet.\n",
                        "2023-09-27 11:22:17,214 :: cappr.openai.api :: INFO :: Try 1. Sleeping for 10 sec.\n"
                    ]
                }
            ],
            "source": [
                "# $0.04\n",
                "# can take a while, ~6 minutes!\n",
                "# idk and idc yet how to batch for the chat endpoint. For correctness, I'll just send\n",
                "# texts 1-by-1\n",
                "choices_chat = openai.api.gpt_chat_complete(\n",
                "    df[\"prompt_mc\"], ask_if_ok=True, max_tokens=5, system_msg=system_prompt_copa\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "completions_chat = pd.Series(\n",
                "    [choice[\"message\"][\"content\"] for choice in choices_chat], index=df.index\n",
                ")\n",
                "\n",
                "pred_classes_chat = pd.Series(\n",
                "    [process_completion(completion, class_chars) for completion in completions_chat],\n",
                "    index=df.index,\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As usual, we need to check that completions are valid."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "1.0"
                        ]
                    },
                    "execution_count": 31,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "mask_valid = pred_classes_chat != -1\n",
                "mask_valid.mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What do invalid completions look like?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Series([], dtype: object)"
                        ]
                    },
                    "execution_count": 32,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "completions_chat[~mask_valid]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What's the accuracy on all completions?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.912"
                        ]
                    },
                    "execution_count": 33,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_classes_chat == df[\"label\"]).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What's the accuracy on *valid* completions?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.912"
                        ]
                    },
                    "execution_count": 34,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_classes_chat[mask_valid] == df.loc[mask_valid, \"label\"]).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluate question"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There are different ways to format a prompt-completion problem. Since `gpt-3.5-turbo-instruct` was trained w/ RLHF, it's worth asking whether a more RLHF-type of prompt would work better. Let's see how performance changes by formatting the problem as a question:\n",
                "\n",
                "```\n",
                "The man broke his toe. What was the cause of this? \n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<style type=\"text/css\">\n",
                            "#T_22bd2_row0_col0, #T_22bd2_row0_col1, #T_22bd2_row0_col2, #T_22bd2_row0_col3, #T_22bd2_row1_col0, #T_22bd2_row1_col1, #T_22bd2_row1_col2, #T_22bd2_row1_col3, #T_22bd2_row2_col0, #T_22bd2_row2_col1, #T_22bd2_row2_col2, #T_22bd2_row2_col3 {\n",
                            "  text-align: left;\n",
                            "  white-space: pre-wrap;\n",
                            "}\n",
                            "</style>\n",
                            "<table id=\"T_22bd2\">\n",
                            "  <thead>\n",
                            "    <tr>\n",
                            "      <th class=\"blank level0\" >&nbsp;</th>\n",
                            "      <th id=\"T_22bd2_level0_col0\" class=\"col_heading level0 col0\" >prompt_question</th>\n",
                            "      <th id=\"T_22bd2_level0_col1\" class=\"col_heading level0 col1\" >choice1</th>\n",
                            "      <th id=\"T_22bd2_level0_col2\" class=\"col_heading level0 col2\" >choice2</th>\n",
                            "      <th id=\"T_22bd2_level0_col3\" class=\"col_heading level0 col3\" >label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th id=\"T_22bd2_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
                            "      <td id=\"T_22bd2_row0_col0\" class=\"data row0 col0\" >My body cast a shadow over the grass. What was the cause of this?</td>\n",
                            "      <td id=\"T_22bd2_row0_col1\" class=\"data row0 col1\" >The sun was rising.</td>\n",
                            "      <td id=\"T_22bd2_row0_col2\" class=\"data row0 col2\" >The grass was cut.</td>\n",
                            "      <td id=\"T_22bd2_row0_col3\" class=\"data row0 col3\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_22bd2_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
                            "      <td id=\"T_22bd2_row1_col0\" class=\"data row1 col0\" >The woman tolerated her friend's difficult behavior. What was the cause of this?</td>\n",
                            "      <td id=\"T_22bd2_row1_col1\" class=\"data row1 col1\" >The woman knew her friend was going through a hard time.</td>\n",
                            "      <td id=\"T_22bd2_row1_col2\" class=\"data row1 col2\" >The woman felt that her friend took advantage of her kindness.</td>\n",
                            "      <td id=\"T_22bd2_row1_col3\" class=\"data row1 col3\" >0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_22bd2_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
                            "      <td id=\"T_22bd2_row2_col0\" class=\"data row2 col0\" >The women met for coffee. What was the cause of this?</td>\n",
                            "      <td id=\"T_22bd2_row2_col1\" class=\"data row2 col1\" >The cafe reopened in a new location.</td>\n",
                            "      <td id=\"T_22bd2_row2_col2\" class=\"data row2 col2\" >They wanted to catch up with each other.</td>\n",
                            "      <td id=\"T_22bd2_row2_col3\" class=\"data row2 col3\" >1</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<pandas.io.formats.style.Styler at 0x12ca48350>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "def prompt_question(premise: str, question: Literal[\"cause\", \"effect\"]):\n",
                "    if question == \"cause\":\n",
                "        question_ = \"What was the cause of this?\"\n",
                "    elif question == \"effect\":\n",
                "        question_ = \"What happened as a result?\"\n",
                "    else:\n",
                "        raise ValueError(\"question must be 'cause' or 'effect'. Got \" f\"{question}.\")\n",
                "    return f\"{premise} {question_}\"\n",
                "\n",
                "\n",
                "df[\"prompt_question\"] = [\n",
                "    prompt_question(premise, question)\n",
                "    for premise, question in zip(df[\"premise\"], df[\"question\"])\n",
                "]\n",
                "\n",
                "\n",
                "display_df(df, columns=[\"prompt_question\", \"choice1\", \"choice2\", \"label\"])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "According to the [docs](https://platform.openai.com/docs/guides/fine-tuning/data-formatting), best practice is to separate prompts and completions using this string:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'\\n\\n###\\n\\n'"
                        ]
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "openai.api.end_of_prompt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "examples_question = [\n",
                "    Example(\n",
                "        prompt=record[\"prompt_question\"],\n",
                "        completions=(record[\"choice1\"], record[\"choice2\"]),\n",
                "        prior=None,\n",
                "        end_of_prompt=openai.api.end_of_prompt,\n",
                "    )\n",
                "    for record in df.to_dict(\"records\")\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "06f21bc7fa1f44c9a650bfdfbdba8ad8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "log-probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# $0.03\n",
                "pred_probs_question = openai.classify.predict_proba_examples(\n",
                "    examples_question, model=\"gpt-3.5-turbo-instruct\", ask_if_ok=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.818"
                        ]
                    },
                    "execution_count": 39,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_probs_question.argmax(axis=1) == df[\"label\"]).mean()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Bad!"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluate single-token"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see how the single-token transformation performs for COPA. Based on the [Evaluate CVS](#evaluate-cvs) result, my hypothesis is that it'll perform slightly better than the [multi-token approach](#run-model). I wouldn't be bummed if it performed better. B/c if I could control the backend, there's still a usability and computational benefit to the idea of returning probabilities for A and B instead of sampling from all possible token sequences."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "examples_mc = [\n",
                "    Example(\n",
                "        prompt=record[\"prompt_mc\"],\n",
                "        completions=(\"A\", \"B\"),\n",
                "        prior=None,\n",
                "        end_of_prompt=openai.api.end_of_prompt,\n",
                "    )\n",
                "    for record in df.to_dict(\"records\")\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c6ce430aa59944c68dd156e46ccd6c6e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "log-probs:   0%|          | 0/1000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# $0.05\n",
                "# If I could control the backend, the cost would be $0.05/2 = $0.025\n",
                "pred_probs_mc = openai.classify.predict_proba_examples(\n",
                "    examples_mc, model=\"gpt-3.5-turbo-instruct\", ask_if_ok=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.958"
                        ]
                    },
                    "execution_count": 42,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(pred_probs_mc.argmax(axis=1) == df[\"label\"]).mean()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Conclusion: performs 5% better than the [multi-token approach](#run-model). I wonder how\n",
                "much better it'd be if we could use the `<|endoftext|>` special token which separate\n",
                "prompts from completion. That could be impactful b/c that was used during training.\n",
                "\n",
                "It costs twice as much. But that's an artifact of the way the endpoint works. For\n",
                "prompts like this, it takes 1 `model()` call to give us the data we need: the\n",
                "probability distribution of (single tokens) `'A'` and `'B'` conditional on the prompt."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "lmc",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "68daa88f78f5c448099edb3a6d3dee27486a6add8824ae1cbe4c903ef8faec70"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
