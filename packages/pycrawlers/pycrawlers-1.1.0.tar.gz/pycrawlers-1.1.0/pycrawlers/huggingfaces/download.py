# -*- coding: UTF-8 -*-
# @Time : 2022/8/17 15:38 
# @Author : åˆ˜æ´ªæ³¢

"""
è·å–hugging face æ¨¡å‹
"""
import random
import os
import time
from lxml import etree
from pycrawlers.common.default_data import headers
from pycrawlers.common.tools import get_session
from pycrawlers.common.tools import DealException
from pycrawlers.common.tools import download
from pycrawlers.common.tools import juedge_path
from pycrawlers.common.tools import juedge_url


class HuggingFace(object):
    def __init__(self, base_url: str = None, token: str = None):
        self.base_url = base_url if base_url else 'https://huggingface.co'
        self.tag = True if base_url else False
        self.html_data = None
        self.requests_session = get_session()
        self.headers = headers.copy()
        if token:
            self.headers['Cookie'] = f'token={token}'

    def get_data(self, url: str, file_save_path: str = None, max_retries: int = 3):
        """
        è·å–å•ä¸ªæ•°æ®
        :param url: ä¾‹ï¼š'htt://huggingface.co/albert-base-v2/tree/main'
        :param file_save_path: None or './albert-base-v2'
        :param max_retries: request æœ€å¤§é‡è¯•æ¬¡æ•°
        :return:
        """
        juedge_url(url)
        _url = url.split('/')
        if not self.tag:
            self.get_base_url(_url)
        response = self.crawl_html(url)
        if response:
            self.html_data = etree.HTML(response.content)
            # print(self.html_data)
            file_names = self.get_file_names()
            file_urls = self.get_file_urls()
            # print(file_urls)
            # print(file_names)
            files_path = juedge_path(file_save_path) if file_save_path else juedge_path('./' + _url[-3] + '/')
            print(f"{'ğŸ”´' * 10}{' ' * 5}Start downloading: {_url[-3]}{' ' * 5}{'ğŸ”´' * 10}")
            self.get_files(file_names, file_urls, files_path, max_retries)
            print(f"{'ğŸŸ¢' * 10}{' ' * 5}Download completed{' ' * 5}{'ğŸŸ¢' * 10}")

    def get_batch_data(self, urls: list, file_save_paths: list = None, count_info=True, max_retries: int = 3):
        """
        æ‰¹é‡è·å–æ•°æ®
        :param urls: ['https://huggingface.co/albert-base-v2/tree/main',
                      'https://huggingface.co/dmis-lab/biosyn-sapbert-bc5cdr-disease/tree/main']
        :param file_save_paths:['./model_1/albert-base-v2', './model_2/']
        :param count_info: æ˜¯å¦ç”Ÿæˆç¨‹åºæ‰§è¡Œçš„ç»Ÿè®¡ä¿¡æ¯
        :param max_retries: request æœ€å¤§é‡è¯•æ¬¡æ•°
        :return:
        """
        success_urls = []
        fail_urls = []
        if file_save_paths:
            if len(urls) == len(file_save_paths):
                for u, f in zip(urls, file_save_paths):
                    success_urls, fail_urls = self.fault_tolerant(u, success_urls, fail_urls, f, max_retries)
            else:
                raise ValueError('The number of urls and paths is inconsistent')
        else:
            for url in urls:
                success_urls, fail_urls = self.fault_tolerant(url, success_urls, fail_urls, max_retries=max_retries)
        if count_info:
            if success_urls or fail_urls:
                self.count_info(success_urls, fail_urls)

    def fault_tolerant(self, url: str, success_urls: list, fail_urls: list, path: str = None, max_retries: int = 3):
        """å®¹é”™å¤„ç†"""
        try:
            self.get_data(url, path, max_retries)
            success_urls.append(url)
            time.sleep(0.5)
        except Exception as e:
            print(e)
            fail_urls.append(url)
        return success_urls, fail_urls

    def get_base_url(self, _url: list):
        """è·å–åŸºç¡€url"""
        if len(_url) > 5:
            if 'http' in _url[0] and _url[1] == '':
                self.base_url = _url[0] + '//' + _url[2]

    @DealException()
    def crawl_html(self, url):
        """è·å–html"""
        return self.requests_session.get(url, headers=self.headers, timeout=1)

    def get_file_names(self):
        """è·å–æ–‡ä»¶å"""
        xpath = f'//div[@data-target="ViewerIndexTreeList"]/ul/li//a[1]/span[1]/text()'
        return self.html_data.xpath(xpath)

    def get_file_urls(self):
        """è·å–æ–‡ä»¶é“¾æ¥"""
        xpath = f'//div[@data-target="ViewerIndexTreeList"]/ul/li/a[1]/@href'
        return self.html_data.xpath(xpath)

    @staticmethod
    def generate_file_path(_url: list):
        """ç”Ÿæˆè·¯å¾„"""
        return juedge_path('./' + _url[-3] + '/')

    def get_files(self, file_names, file_urls, files_path, max_retries):
        for name, part_url in zip(file_names, file_urls):
            if name in part_url:
                url = self.base_url + part_url
                save_file_path = files_path + name
                download(url, save_file_path, self.headers,
                         read_timeout=60, file_size=self.get_file_size(save_file_path), max_retries=max_retries)
                time.sleep(random.random())

    @staticmethod
    def count_info(success_urls, fail_urls):
        print(f'ç¨‹åºæ‰§è¡Œç»Ÿè®¡ï¼š')
        print(f'a. æˆåŠŸ{str(len(success_urls))}ä¸ª')
        print(f'b. å¤±è´¥{str(len(fail_urls))}ä¸ª')
        print(f'c. æˆåŠŸçš„URLï¼š{"ï¼Œ".join(success_urls)}')
        print(f'd. å¤±è´¥çš„URLï¼š{"ï¼Œ".join(fail_urls)}')

    @staticmethod
    def get_file_size(file_path):
        """
        è·å–æ–‡ä»¶å¤§å°
        :param:  file_path:æ–‡ä»¶è·¯å¾„ï¼ˆå¸¦æ–‡ä»¶åï¼‰
        :return: file_sizeï¼šæ–‡ä»¶å¤§å°
        """
        if os.path.exists(file_path):
            return os.path.getsize(file_path)
        else:
            return 0
