# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['mmca']

package_data = \
{'': ['*']}

install_requires = \
['einops', 'torch']

setup_kwargs = {
    'name': 'mmca',
    'version': '0.0.4',
    'description': 'MMCA - Pytorch',
    'long_description': '[![Multi-Modality](agorabanner.png)](https://discord.gg/qUtxnK2NMf)\n\n# Multi-Modal Causal Attention\nThe open source community\'s implementation of the all-new Multi-Modal Causal Attention from "DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention"\n\n\n[Paper Link](https://arxiv.org/pdf/2309.14327.pdf)\n\n# Appreciation\n* Lucidrains\n* Agorians\n\n\n\n# Install\n`pip install mmca`\n\n# Usage\n```python\nimport torch \nfrom mmca.main import MultiModalCausalAttention\n\n\nattn = MultiModalCausalAttention(dim=512, heads=8)\n\nx = torch.randn(1, 10, 512)\ny = torch.randn(1, 20, 512)\n\nx, y = attn(x, y)\n\nprint(x)\nprint(y)\n```\n\n# Architecture\nAlgorithmic pseudocode\n\n```latex\nInput: Visual tokens V, Textual tokens T\nOutput: Updated Textual tokens T\'\n\n1: procedure MMCA(V, T)\n2:     for each visual token v in V do\n3:         v\' = self_attention(v)  // Visual tokens only attend to themselves\n4:     end for\n5:     for each textual token t in T do\n6:         t\' = attention(t, T_previous) + attention(t, V)  // Textual tokens attend to all their previous tokens AND image tokens\n7:     end for\n8:     return T\'\n9: end procedure\n```\n\n# Multi-Modal Causal Attention: A Study\n\nMMCA is a novel attention mechanism designed to handle multi-modal data, i.e., data that comes from different sources or formats, such as text and images. It is an extension of the causal attention mechanism, which is commonly used in transformer models for tasks like language modeling.\n\n## Causal Attention\n----------------\n\nBefore diving into MMCA, let\'s first understand the concept of causal attention. In the context of transformers, attention is a measure of how much a model should focus on different parts of the input when producing a particular part of the output.\n\nCausal attention, also known as autoregressive or self-attention, is a type of attention where a token can only attend to previous tokens in the sequence. This is in contrast to other types of attention where a token can attend to all other tokens in the sequence.\n\nThe causal attention mechanism can be visualized as follows:\n\n```\nToken1 -> |------|\nToken2 -> |------|------|\nToken3 -> |------|------|------|\nToken4 -> |------|------|------|------|\n\n```\n\nEach token can attend to itself and all the tokens before it, but not the ones after it.\n\n----\n\n## Multi-Modal Causal Attention\n\nIn a multi-modal setting, we often deal with different types of data simultaneously. For instance, in an image captioning task, the model has to process both image features and textual data. This is where MMCA comes into play.\n\nMMCA extends the concept of causal attention to handle multi-modal data. The key idea behind MMCA is as follows:\n\n1.  For visual tokens, they only attend to themselves, as visual tokens are encoded by the visual encoder.\n2.  For textual tokens, they attend to all their previous tokens. However, they have two separate attention weight matrices for their previous textual tokens and image tokens.\n\nThis can be visualized as follows:\n\n```\nVisual Tokens:\nV1 -> |------|\nV2 -> |------|\nV3 -> |------|\n\nTextual Tokens:\nT1 -> |------|------|------|------|\nT2 -> |------|------|------|------|------|\nT3 -> |------|------|------|------|------|------|\n\n```\n\nHere,\xa0`V1`,\xa0`V2`, and\xa0`V3`\xa0are visual tokens, and\xa0`T1`,\xa0`T2`, and\xa0`T3`\xa0are textual tokens. Each visual token only attends to itself, while each textual token attends to all previous textual and visual tokens.\n\n----\n\n## Mathematical Formulation\n\nLet\'s now delve into the mathematical formulation of MMCA. The attention mechanism in transformers is typically computed using the dot product of query\xa0`Q`\xa0and key\xa0`K`\xa0matrices, followed by a softmax operation. In MMCA, we have two separate attention weight matrices for textual and visual tokens.\n\nLet\xa0`Q_T`\xa0and\xa0`K_T`\xa0be the query and key matrices for textual tokens, and\xa0`Q_V`\xa0and\xa0`K_V`\xa0be the query and key matrices for visual tokens. The attention weights for textual tokens attending to previous textual tokens (`A_TT`) and visual tokens (`A_TV`) can be computed as follows:\n\n```\nA_TT = softmax(Q_T * K_T^T)\nA_TV = softmax(Q_T * K_V^T)\n\n```\n\nThe updated textual token representations can then be computed by applying these attention weights to the value\xa0`V`\xa0matrices:\n\n```\nT\' = A_TT * V_T + A_TV * V_V\n\n```\n\nHere,\xa0`V_T`\xa0and\xa0`V_V`\xa0are the value matrices for textual and visual tokens, respectively.\n\n\n## Conclusion\n\nMulti-Modal Causal Attention is a powerful attention mechanism that extends the concept of causal attention to handle multi-modal data. It allows a model to process different types of data simultaneously and in a more efficient manner. By having separate attention weight matrices for different types of tokens, MMCA allows the model to focus on the most relevant parts of the input for each type of token, leading to improved performance on multi-modal tasks.\n\n\n---\n\n# Todo\n* implement flash attention from zeta as the main attn\n---\n\n# License\nMIT\n\n---\n\n# Citations\n```bibtex\n@misc{2309.14327,\nAuthor = {Zhewei Yao and Xiaoxia Wu and Conglong Li and Minjia Zhang and Heyang Qi and Olatunji Ruwase and Ammar Ahmad Awan and Samyam Rajbhandari and Yuxiong He},\nTitle = {DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention},\nYear = {2023},\nEprint = {arXiv:2309.14327},\n}\n```',
    'author': 'Kye Gomez',
    'author_email': 'kye@apac.ai',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'https://github.com/kyegomez/MMCA',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.6,<4.0',
}


setup(**setup_kwargs)
