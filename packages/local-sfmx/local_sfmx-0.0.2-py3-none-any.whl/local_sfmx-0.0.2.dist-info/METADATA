Metadata-Version: 2.1
Name: local-sfmx
Version: 0.0.2
Summary: local-sftmx - Pytorch
Home-page: https://github.com/kyegomez/LocalSoftmax
License: MIT
Keywords: artificial intelligence,deep learning,optimizers,Prompt Engineering
Author: Kye Gomez
Author-email: kye@apac.ai
Requires-Python: >=3.6,<4.0
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.6
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Dist: torch
Project-URL: Repository, https://github.com/kyegomez/LocalSoftmax
Description-Content-Type: text/markdown

[![Multi-Modality](agorabanner.png)](https://discord.gg/qUtxnK2NMf)

# LocalSoftmax
Local Softmax parallelize the softmax computation by splitting the tensor into smaller sub-tensors and applying the softmax function on each of these smaller tensors independently. In other words, we want to compute a "local" softmax on each chunk of the tensor, instead of on the entire tensor.

# Appreciation
* Lucidrains
* Agorians



# Install
`pip install local-sftmx`


## Usage
```python
import torch
from local_sfmx import local_softmax

tensor = torch.rand(10, 5)
result = local_softmax(tensor, 2)
print(result)
```

# Algorithm
function LocalSoftmax(tensor, num_chunks):
    split tensors into `num_chunks` smaller tensors
    for each smaller tensor:
        apply standard softmax
    concatenate the results
    return concatenated tensor

# License
MIT


