Metadata-Version: 2.1
Name: xllm
Version: 0.2.0
Summary: Simple & Cutting Edge LLM Finetuning
Home-page: https://github.com/kompleteai/xllm
Author: BobaZooba
Author-email: bobazooba@gmail.com
License: Apache
Keywords: ai nlp llm text deep-learning
Platform: UNKNOWN
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8.0
Description-Content-Type: text/markdown
Requires-Dist: numpy >=1.17
Requires-Dist: packaging >=20.0
Requires-Dist: psutil
Requires-Dist: torch >=2.0.0
Requires-Dist: loguru
Requires-Dist: peft >=0.5.0
Requires-Dist: wandb
Requires-Dist: python-dotenv
Requires-Dist: requests
Requires-Dist: optimum >=1.12.0
Requires-Dist: bitsandbytes >=0.41.1
Requires-Dist: scipy
Requires-Dist: transformers
Requires-Dist: tqdm
Requires-Dist: safetensors
Provides-Extra: all
Requires-Dist: deepspeed ; extra == 'all'
Requires-Dist: flash-attn >=2.2.1 ; extra == 'all'
Requires-Dist: auto-gptq ; extra == 'all'
Requires-Dist: black ; extra == 'all'
Requires-Dist: mypy ; extra == 'all'
Requires-Dist: mypy-extensions ; extra == 'all'
Requires-Dist: pre-commit ; extra == 'all'
Requires-Dist: ruff ; extra == 'all'
Requires-Dist: pytest ; extra == 'all'
Requires-Dist: coverage ; extra == 'all'
Requires-Dist: pytest-html ; extra == 'all'
Requires-Dist: pytest-cov ; extra == 'all'
Provides-Extra: auto-gptq
Requires-Dist: auto-gptq ; extra == 'auto-gptq'
Provides-Extra: deepspeed
Requires-Dist: deepspeed ; extra == 'deepspeed'
Provides-Extra: dev
Requires-Dist: black ; extra == 'dev'
Requires-Dist: mypy ; extra == 'dev'
Requires-Dist: mypy-extensions ; extra == 'dev'
Requires-Dist: pre-commit ; extra == 'dev'
Requires-Dist: ruff ; extra == 'dev'
Requires-Dist: pytest ; extra == 'dev'
Requires-Dist: coverage ; extra == 'dev'
Requires-Dist: pytest-html ; extra == 'dev'
Requires-Dist: pytest-cov ; extra == 'dev'
Provides-Extra: flash-attn
Requires-Dist: flash-attn >=2.2.1 ; extra == 'flash-attn'
Provides-Extra: quality
Requires-Dist: black ; extra == 'quality'
Requires-Dist: mypy ; extra == 'quality'
Requires-Dist: mypy-extensions ; extra == 'quality'
Requires-Dist: pre-commit ; extra == 'quality'
Requires-Dist: ruff ; extra == 'quality'
Provides-Extra: tests
Requires-Dist: pytest ; extra == 'tests'
Requires-Dist: coverage ; extra == 'tests'
Requires-Dist: pytest-html ; extra == 'tests'
Requires-Dist: pytest-cov ; extra == 'tests'
Provides-Extra: train
Requires-Dist: deepspeed ; extra == 'train'
Requires-Dist: flash-attn >=2.2.1 ; extra == 'train'
Requires-Dist: auto-gptq ; extra == 'train'

# ðŸ¦– Xâ€”LLM: Simple & Cutting Edge LLM Finetuning

<div align="center">

[![Build](https://github.com/KompleteAI/xllm/actions/workflows/build.yaml/badge.svg?branch=main)](https://github.com/KompleteAI/xllm/actions/workflows/build.yaml)
[![Github: License](https://img.shields.io/github/license/KompleteAI/xllm.svg?color=blue)](https://github.com/KompleteAI/xllm/blob/main/LICENSE)
[![Github: Release](https://img.shields.io/github/v/release/kompleteai/xllm.svg)](https://github.com/KompleteAI/xllm/releases)
[![Pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/modelfront/predictor/blob/master/.pre-commit-config.yaml)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Checked with mypy](http://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![codecov](https://codecov.io/gh/KompleteAI/xllm/graph/badge.svg?token=ZOBMDXVW4B)](https://codecov.io/gh/KompleteAI/xllm)

Simple & cutting edge LLM finetuning using the most advanced methods (QLoRA, DeepSpeed, GPTQ, Flash Attention 2, FSDP,
etc)

</div>

## Features

- Train LLM's easily
- Simply add QLoRA, DeepSpeed
- Track your progress using W&B
- Easy to extend and integrate to your project

## Before we start

## Table of Contents

# What problem does Xâ€”LLM solve?

## Efficient finetune

See benchmark later

## Fusing LoRA

## GPTQ Quantization

## Extend with your data, collator and trainer

# Projects using Xâ€”LLM

Shurale
some

## Quickstart âš¡

## Install

## Examples

### Finetune Llama 7b

### Finetune Falcon 180b

# How to use in your project

## Prepare

## Data

## Collator

## Trainer

# Training Benchmark

> RTX 4090 have 24 Gb, so 4 RTX 4090 have 24 Gb * 4 = 96 Gb

## Full Finetune

<details>
<summary>Single RTX 4090, Full Finetune</summary>
</details>

<details>
<summary>4 RTX 4090, Full Finetune, DeepSpeed Stage 1</summary>
</details>

<details>
<summary>4 RTX 4090, Full Finetune, DeepSpeed Stage 2</summary>
</details>

<details>
<summary>4 RTX 4090, Full Finetune, DeepSpeed Stage 3</summary>
</details>

<details>
<summary>4 RTX 4090, Full Finetune, FSDP</summary>
</details>

## LoRA

<details>
<summary>Single RTX 4090, LoRA</summary>
</details>

<details>
<summary>4 RTX 4090, LoRA, DeepSpeed Stage 1</summary>
</details>

<details>
<summary>4 RTX 4090, LoRA, DeepSpeed Stage 2</summary>
</details>

<details>
<summary>4 RTX 4090, LoRA, DeepSpeed Stage 3</summary>
</details>

<details>
<summary>4 RTX 4090, LoRA, FSDP</summary>
</details>

# Badge

Building something cool with Xâ€”LLM? Consider adding a badge to your model card.

```bash
[<img src="https://github.com/KompleteAI/xllm/blob/main/static/images/xllm-badge.png" alt="Powered by Xâ€”LLM" width="175" height="32"/>](https://github.com/KompleteAI/xllm)
```

[<img src="https://github.com/KompleteAI/xllm/blob/main/static/images/xllm-badge.png" alt="Powered by Xâ€”LLM" width="175" height="32"/>](https://github.com/KompleteAI/xllm)

# Lack of tests

# Future Work

- Add more tests
- GPU CI using RunPod
- Add multipacking

## Call to action

If you like this library, please help me find a job


